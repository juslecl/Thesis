---
title: "Thesis : R"
author: "Justine Leclerc"
date: "2024-09-06"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(splines)
library(stats4)
library(VGAM)
library(openxlsx)
```

### Generating random data process.
```{r echo=FALSE}
set.seed(123)
#1) create a dataset with X following a normal but different patterns for the error.
datagen <- function(n, beta, rcdf, meanX, sdX, M=100) {
  d <- length(meanX)
  datasets <- vector("list", M)  # Create a list to store M datasets
  for (m in 1:M) {  # Loop over the number of replicates
    x <- matrix(data = NA, nrow = n, ncol = d)  # Design matrix
    for (j in 1:d)  x[,j] <- rnorm(n,mean=meanX[j],sd=sdX[j])
    eps <- rcdf(n)  # Generate epsilon
    y <- x%*%beta + eps  # Response variable
    datasets[[m]] <- list(Y = y, X = x, B = beta, E = eps)  # Store dataset in the list
}
  return(datasets)  # Return the list of datasets
}
```

### Formula for criterion
```{r echo=FALSE}
# Differentiable form
critbis <- function(beta, data, F_eps) {
  n <- length(data$Y)
  residuals <- data$Y - data$X%*%beta  # This gives a vector of length n
  F_eps_residuals <- F_eps(as.vector(residuals))  # Ensure itâ€™s a vector
  p2 <- mean(F_eps_residuals^2)
  p3 <- (1 / (n^2)) * sum(sapply(1:n, function(i) {
    sum(sapply(1:n, function(j) {
      if (i != j) {
        return(max(F_eps_residuals[i], F_eps_residuals[j]))
      } else {
        return(0)
      }
    }))
  }))
  p4 <- (1 / (n^2)) * sum(F_eps_residuals)
  result <- 1/3 + p2 - p3 - p4
  return(result)
}
```

### Gradient for the criterion.
```{r include=FALSE}
# Implement the gradient. Returns a vector in R^d and the corresponding L-2 norm.
grad <- function(data, beta, F_eps,f_eps) {
  n <- length(data$Y)
  d <- length(beta)
  residuals <- data$Y - data$X%*%beta
  f_residuals <- f_eps(residuals) #returns a nx1 matrix
  F_residuals <- F_eps(residuals) #returns a nx1 matrix
  term1_weights <- f_residuals * (1 - 2 * F_residuals) # returns a 1xn matrix.
  term1 <- (t(term1_weights)%*%data$X) / n #1xd matrix.
    term2 <- 0
  for (i in 1:n) {
    for (j in 1:n) {
      if (i != j) {
        r_i <- residuals[i]
        r_j <- residuals[j]
        indicator <- as.numeric(r_i > r_j)
        term2 <- term2 + indicator * (f_eps(r_i) * data$X[i,] - f_eps(r_j) * data$X[i,])
      }
    }
  }
  term2 <- term2 / n^2
  result <- term1 + term2
  norm <- sqrt(sum(result^2))
  return(list(G=result,N=norm)) #$G is 1xd matrix.
}
```

# Gradient descent algorithm.
```{r echo=FALSE}
# Function to build OLS.
ols <- function(data) {
  return(solve(t(data$X)%*%data$X)%*%t(data$X)%*%data$Y)
}

# Gradient with Backtracking Line Search
# Since the initial values are crucial for the performance of the algorithm, take as initial value classic OLS.
# Try to improve the speed 
backtracking<-function(data, cdf, pmf, tol = 1e-3, a = .001, b = .9,max_iteration = 1000){
  k = 1
  beta <- ols(data) #returns a 2x1 matrix.
  betaplus <- beta - a * t(grad(data, beta, cdf,pmf)$G/grad(data, beta, cdf,pmf)$N)
  while (sqrt(sum((beta - betaplus)^2)) > tol && k < max_iteration){
    a <- b * a
    beta <- betaplus
    betaplus <- beta - a * t(grad(data, beta, cdf,pmf)$G/grad(data, beta, cdf,pmf)$N)
    k <- k + 1
  }
  return(list(iteration = k - 1, res = betaplus, compa = cbind(data$B,betaplus,ols(data))))
}
```

### Simulations for || \beta^*-\beta_0||, || OLS-\beta_0||
```{r echo=FALSE}
# Create a function which returns as a list both || \beta^*-\beta_0|| and || OLS-\beta_0||.
normbeta <- function(data, F_eps, f_eps) {
  return(list(norm1=sqrt(sum((backtracking(data,F_eps, f_eps)$res-data$B)^2)),norm2=sqrt(sum((data$B-ols(data))^2))))
}
```



# Compute a Laplace MLE.
```{r}
library(optimx)

datplacexl <- datagen(500, c(3.75,1.6,0.3), rlaplace, c(5,3,-2), c(1.3,0.5,2), M=100)

# MLE algorithm.
# Define the negative log-likelihood function for Laplace-distributed errors
neg_lllaplace <- function(params, X, Y) {
  beta <- params # Extract regression coefficients (beta)
  b <- 1  # Extract the scale parameter (b)
  
  # Calculate residuals (errors) from the linear model
  residuals <- Y - X %*% beta
  
  # Negative log-likelihood for Laplace distribution
  n <- length(Y)
  neg_ll <- n * log(2 * b) + sum(abs(residuals) / b)
  
  return(neg_ll)
}

initial_params <- c(rep(0, ncol(datplacexl[[1]]$X)))
# Optimize the negative log-likelihood
fit <- optimx(par=initial_params, fn=neg_lllaplace, X=datplacexl[[1]]$X, Y=datplacexl[[1]]$Y, method="BFGS")

mledatplacexl <- lapply(datplacexl, function(data) optimx(par=initial_params, fn=neg_lllaplace, X=data$X, Y=data$Y, method="BFGS"))

mlelaplace <- matrix(rep(0,300),nrow=100)
for (i in 1:100){
  mlelaplace[i,] <-c(mledatplacexl[[i]]$p1,mledatplacexl[[i]]$p2,mledatplacexl[[i]]$p3)
}
mleplaplace

normmle <- apply(mlelaplace,1,function(data) sqrt(sum(data-c(3.75,1.6,0.3))^2))

neg_llcauchy <- function(params, X, Y) {
  beta <- params  # Extract regression coefficients (beta)
  gamma <- 1  # Scale parameter (gamma) - fixed here, but could be made a parameter to optimize
  
  # Calculate residuals (errors) from the linear model
  residuals <- Y - X %*% beta
  
  # Negative log-likelihood for Cauchy distribution
  n <- length(Y)
  neg_ll <- n * log(pi * gamma) + sum(log(1 + (residuals / gamma)^2))
  
  return(neg_ll)
}


#file_path <- "~/polybox - Justine Leclerc (jleclerc@student.ethz.ch)@polybox.ethz.ch (2)/THESIS !/res_simul.xlsx"
#wb <- wb <- loadWorkbook(file_path)
#addWorksheet(wb, "1002laplace")
#writeData(wb, sheet = "1002laplace", resnorm3)
#saveWorkbook(wb, file_path, overwrite = TRUE)
```

## Results to be added in xslx.
```{r}
# datagen(1500,c(-3,2,0.3,0.8,-6),function(x) rt(x,df=2),c(0.4,-4,1,2.3,1.75),c(0.23,3,0.75,1,2))
[[1]]
[[1]]$norm1
[1] 0.3024049

[[1]]$norm2
[1] 0.3024733


[[2]]
[[2]]$norm1
[1] 0.1079507

[[2]]$norm2
[1] 0.1082892


[[3]]
[[3]]$norm1
[1] 0.2595023

[[3]]$norm2
[1] 0.2597971


[[4]]
[[4]]$norm1
[1] 0.1439904

[[4]]$norm2
[1] 0.1442997


[[5]]
[[5]]$norm1
[1] 0.1677116

[[5]]$norm2
[1] 0.1678504


[[6]]
[[6]]$norm1
[1] 0.05724146

[[6]]$norm2
[1] 0.05785145


[[7]]
[[7]]$norm1
[1] 0.3412128

[[7]]$norm2
[1] 0.3418773


[[8]]
[[8]]$norm1
[1] 0.1366754

[[8]]$norm2
[1] 0.1367868


[[9]]
[[9]]$norm1
[1] 0.1825214

[[9]]$norm2
[1] 0.1832797


[[10]]
[[10]]$norm1
[1] 0.1663917

[[10]]$norm2
[1] 0.1668143


[[11]]
[[11]]$norm1
[1] 0.418041

[[11]]$norm2
[1] 0.4188546


[[12]]
[[12]]$norm1
[1] 0.2284368

[[12]]$norm2
[1] 0.2286371


[[13]]
[[13]]$norm1
[1] 0.3994588

[[13]]$norm2
[1] 0.399971


[[14]]
[[14]]$norm1
[1] 0.09039952

[[14]]$norm2
[1] 0.09114374


[[15]]
[[15]]$norm1
[1] 0.0855475

[[15]]$norm2
[1] 0.08594391


[[16]]
[[16]]$norm1
[1] 0.2116405

[[16]]$norm2
[1] 0.211738


[[17]]
[[17]]$norm1
[1] 0.2403852

[[17]]$norm2
[1] 0.2406708


[[18]]
[[18]]$norm1
[1] 0.3109217

[[18]]$norm2
[1] 0.3113241


[[19]]
[[19]]$norm1
[1] 0.5571462

[[19]]$norm2
[1] 0.5577259


[[20]]
[[20]]$norm1
[1] 0.1047026

[[20]]$norm2
[1] 0.104931


[[21]]
[[21]]$norm1
[1] 0.1530909

[[21]]$norm2
[1] 0.1534681


[[22]]
[[22]]$norm1
[1] 0.1822983

[[22]]$norm2
[1] 0.1828098


[[23]]
[[23]]$norm1
[1] 0.4528682

[[23]]$norm2
[1] 0.4529412


[[24]]
[[24]]$norm1
[1] 0.2149322

[[24]]$norm2
[1] 0.2152077


[[25]]
[[25]]$norm1
[1] 0.2266744

[[25]]$norm2
[1] 0.2270573


[[26]]
[[26]]$norm1
[1] 0.214937

[[26]]$norm2
[1] 0.2154852


[[27]]
[[27]]$norm1
[1] 0.08508175

[[27]]$norm2
[1] 0.0853753


[[28]]
[[28]]$norm1
[1] 0.1816558

[[28]]$norm2
[1] 0.1822461


[[29]]
[[29]]$norm1
[1] 0.1637903

[[29]]$norm2
[1] 0.1638985


[[30]]
[[30]]$norm1
[1] 0.2733342

[[30]]$norm2
[1] 0.2735791


[[31]]
[[31]]$norm1
[1] 0.3140388

[[31]]$norm2
[1] 0.3141639


[[32]]
[[32]]$norm1
[1] 0.2193085

[[32]]$norm2
[1] 0.2196261


[[33]]
[[33]]$norm1
[1] 0.112665

[[33]]$norm2
[1] 0.1129824


[[34]]
[[34]]$norm1
[1] 0.1028426

[[34]]$norm2
[1] 0.1031233


[[35]]
[[35]]$norm1
[1] 0.05411442

[[35]]$norm2
[1] 0.05488552


[[36]]
[[36]]$norm1
[1] 0.1469831

[[36]]$norm2
[1] 0.1470961


[[37]]
[[37]]$norm1
[1] 0.935536

[[37]]$norm2
[1] 0.9356365


[[38]]
[[38]]$norm1
[1] 0.4416015

[[38]]$norm2
[1] 0.4425201


[[39]]
[[39]]$norm1
[1] 0.1513665

[[39]]$norm2
[1] 0.151624


[[40]]
[[40]]$norm1
[1] 0.7466081

[[40]]$norm2
[1] 0.7473054


[[41]]
[[41]]$norm1
[1] 0.07872246

[[41]]$norm2
[1] 0.07976022


[[42]]
[[42]]$norm1
[1] 0.103685

[[42]]$norm2
[1] 0.1045331


[[43]]
[[43]]$norm1
[1] 0.8389124

[[43]]$norm2
[1] 0.8392433


[[44]]
[[44]]$norm1
[1] 0.08562397

[[44]]$norm2
[1] 0.08562779


[[45]]
[[45]]$norm1
[1] 0.5577779

[[45]]$norm2
[1] 0.5578778


[[46]]
[[46]]$norm1
[1] 0.2854983

[[46]]$norm2
[1] 0.2855825


[[47]]
[[47]]$norm1
[1] 0.3107976

[[47]]$norm2
[1] 0.3111089


[[48]]
[[48]]$norm1
[1] 0.09592559

[[48]]$norm2
[1] 0.09562243


[[49]]
[[49]]$norm1
[1] 0.07576152

[[49]]$norm2
[1] 0.07646748


[[50]]
[[50]]$norm1
[1] 0.1095815

[[50]]$norm2
[1] 0.1101039


[[51]]
[[51]]$norm1
[1] 0.2636719

[[51]]$norm2
[1] 0.2640194


[[52]]
[[52]]$norm1
[1] 0.1569709

[[52]]$norm2
[1] 0.1570409


[[53]]
[[53]]$norm1
[1] 0.3166693

[[53]]$norm2
[1] 0.3169012


[[54]]
[[54]]$norm1
[1] 0.1246365

[[54]]$norm2
[1] 0.1253436


[[55]]
[[55]]$norm1
[1] 0.05898362

[[55]]$norm2
[1] 0.05931794


[[56]]
[[56]]$norm1
[1] 0.2314477

[[56]]$norm2
[1] 0.2320451


[[57]]
[[57]]$norm1
[1] 0.1678422

[[57]]$norm2
[1] 0.1680699


[[58]]
[[58]]$norm1
[1] 0.1932207

[[58]]$norm2
[1] 0.1946953


[[59]]
[[59]]$norm1
[1] 0.1773543

[[59]]$norm2
[1] 0.1776331


[[60]]
[[60]]$norm1
[1] 0.2901821

[[60]]$norm2
[1] 0.2905397


[[61]]
[[61]]$norm1
[1] 0.3636929

[[61]]$norm2
[1] 0.363924


[[62]]
[[62]]$norm1
[1] 0.1643818

[[62]]$norm2
[1] 0.1656759


[[63]]
[[63]]$norm1
[1] 0.1772068

[[63]]$norm2
[1] 0.1773338


[[64]]
[[64]]$norm1
[1] 0.6199444

[[64]]$norm2
[1] 0.6200585


[[65]]
[[65]]$norm1
[1] 0.2602716

[[65]]$norm2
[1] 0.2604092


[[66]]
[[66]]$norm1
[1] 0.1863814

[[66]]$norm2
[1] 0.1864444


[[67]]
[[67]]$norm1
[1] 0.22675

[[67]]$norm2
[1] 0.2269201


[[68]]
[[68]]$norm1
[1] 0.2303804

[[68]]$norm2
[1] 0.2306993


[[69]]
[[69]]$norm1
[1] 0.1424655

[[69]]$norm2
[1] 0.1432879


[[70]]
[[70]]$norm1
[1] 0.3152816

[[70]]$norm2
[1] 0.3154081


[[71]]
[[71]]$norm1
[1] 0.1952925

[[71]]$norm2
[1] 0.1958203


[[72]]
[[72]]$norm1
[1] 0.1236274

[[72]]$norm2
[1] 0.1247335


[[73]]
[[73]]$norm1
[1] 0.2534822

[[73]]$norm2
[1] 0.2536323


[[74]]
[[74]]$norm1
[1] 0.280199

[[74]]$norm2
[1] 0.2803332


[[75]]
[[75]]$norm1
[1] 0.1120688

[[75]]$norm2
[1] 0.1126648


[[76]]
[[76]]$norm1
[1] 0.4900164

[[76]]$norm2
[1] 0.4900899


[[77]]
[[77]]$norm1
[1] 0.1952173

[[77]]$norm2
[1] 0.1952358


[[78]]
[[78]]$norm1
[1] 0.3990552

[[78]]$norm2
[1] 0.3991954


[[79]]
[[79]]$norm1
[1] 0.1118158

[[79]]$norm2
[1] 0.1132099


[[80]]
[[80]]$norm1
[1] 0.2449569

[[80]]$norm2
[1] 0.2454702


[[81]]
[[81]]$norm1
[1] 0.1653214

[[81]]$norm2
[1] 0.1656912


[[82]]
[[82]]$norm1
[1] 0.3123922

[[82]]$norm2
[1] 0.3125222


[[83]]
[[83]]$norm1
[1] 0.06584454

[[83]]$norm2
[1] 0.06647552


[[84]]
[[84]]$norm1
[1] 0.3340022

[[84]]$norm2
[1] 0.3341693


[[85]]
[[85]]$norm1
[1] 0.5109378

[[85]]$norm2
[1] 0.511125


[[86]]
[[86]]$norm1
[1] 0.1001854

[[86]]$norm2
[1] 0.1006792


[[87]]
[[87]]$norm1
[1] 0.369775

[[87]]$norm2
[1] 0.3702196


[[88]]
[[88]]$norm1
[1] 0.4183519

[[88]]$norm2
[1] 0.4187879


[[89]]
[[89]]$norm1
[1] 0.6601393

[[89]]$norm2
[1] 0.6602934


[[90]]
[[90]]$norm1
[1] 0.2928314

[[90]]$norm2
[1] 0.2935606


[[91]]
[[91]]$norm1
[1] 0.3476073

[[91]]$norm2
[1] 0.3478943


[[92]]
[[92]]$norm1
[1] 0.3025638

[[92]]$norm2
[1] 0.3028951


[[93]]
[[93]]$norm1
[1] 6.197504

[[93]]$norm2
[1] 6.197713


[[94]]
[[94]]$norm1
[1] 0.2062438

[[94]]$norm2
[1] 0.2063284


[[95]]
[[95]]$norm1
[1] 0.2680734

[[95]]$norm2
[1] 0.2685151


[[96]]
[[96]]$norm1
[1] 0.1498001

[[96]]$norm2
[1] 0.151107


[[97]]
[[97]]$norm1
[1] 0.2443741

[[97]]$norm2
[1] 0.2445175


[[98]]
[[98]]$norm1
[1] 0.6098253

[[98]]$norm2
[1] 0.6100352


[[99]]
[[99]]$norm1
[1] 0.4921494

[[99]]$norm2
[1] 0.4922373


[[100]]
[[100]]$norm1
[1] 1.127735

[[100]]$norm2
[1] 1.127911


```

